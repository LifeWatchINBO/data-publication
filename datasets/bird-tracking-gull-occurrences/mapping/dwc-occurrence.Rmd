---
title: "Darwin Core mapping for bird tracking data"
author: "Peter Desmet"
date: "2017-04-18"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

This document describes how our bird tracking data are standardized to [Darwin Core](https://en.wikipedia.org/wiki/Darwin_Core), so these can be published via [our Integrated Publishing Toolkit (IPT)](http://data.inbo.be/ipt) to the [Global Biodiversity Information Facility (GBIF)](http://www.gbif.org/).

For those familiar with Darwin Core: the data are mapped to an [Occurrence core](http://rs.gbif.org/core/dwc_occurrence_2015-07-02.xml), with every recorded position/time as an occurrence. Individual tracks can be created by filtering on `organismID` and sorting on `eventDate` (done by default).

## Pre-processing

These steps are done by the R package [bird-tracking-etl](https://github.com/inbo/bird-tracking-etl) which runs monthly on AWS:

1. Download INBO bird tracking data (logs) from the UvA-BiTS virtual lab.
2. Process and augment these, including joining the data with the `bird_tracking_devices` metadata, which are managed in [this Google Spreadsheet](https://drive.google.com/open?id=168lV0h4f0uljItStoNX16j8stjJg640ozrgzHW-5d9w).
3. Upload the resulting processed data as `csv` and `sqlite` for registered users at http://birdtracking-downloader.eu-west-1.elasticbeanstalk.com/, as well as the original unprocessed data (as `csv`).

We need the processed data for the further steps, because we want the metadata, be able to filter on outliers (`calc_outlier == FALSE`) and species (`species_code`), and make use of the calculated field `calc_time_diff`.

## Download data

1. Go to http://birdtracking-downloader.eu-west-1.elasticbeanstalk.com/ (login required) and manually download the latest processed data as a zipped csv (e.g. `2017-04-03-processed-logs.csv.gz`) to your Downloads folder. This download tends to be very slow.
2. Unzip the `gz` file.

## Transform data

The following data transformation script was initially created in [Exploratory](https://exploratory.io/), but can be adapted as it is just an R script. Here we describe how to run it in Exploratory, but with minor adaptions it should be able to run elsewhere.

1. Create a new project in Exploratory.
2. Add a Data Frame with the option `Import by writing R script`, give it a name, and copy/paste the following script (the only thing to adapt is the name and location of the source file):

    ```r
    # Define source file
    source_file <- "/Users/peter_desmet/Downloads/2017-04-03-processed-logs.csv"

    # Set libPaths
    .libPaths("/Users/peter_desmet/.exploratory/R/3.3")

    # Load required packages
    library(janitor)
    library(lubridate)
    library(hms)
    library(tidyr)
    library(stringr)
    library(readr)
    library(forcats)
    library(RcppRoll)
    library(dplyr)
    library(exploratory)

    # Load source file
    exploratory::read_delim_file(source_file, ",", quote = "\"", skip = 0 , col_names = TRUE , na = c("","NA") , locale=locale(encoding = "UTF-8", decimal_mark = "."), trim_ws = FALSE , progress = FALSE) %>% exploratory::clean_data_frame() %>%

    # Sort by device_info_serial and date_time
    arrange(device_info_serial) %>%
    arrange(date_time) %>%

    # Map to Darwin Core
    ## Record terms
    mutate(occurrenceID = paste(device_info_serial, format(date_time, "%Y%m%d%H%M%S"), sep = ":")) %>%
    mutate(type = "Event") %>%
    mutate(language = "en") %>%
    mutate(license = "http://creativecommons.org/publicdomain/zero/1.0/") %>%
    mutate(rightsHolder = "INBO") %>%
    mutate(accessRights = "http://www.inbo.be/en/norms-for-data-use") %>%
    mutate(datasetID = case_when(.$species_code %in% c("lbbg", "hg") ~ "https://doi.org/10.15468/02omly", .$species_code == "wmh" ~ "https://doi.org/10.15468/rbguhj")) %>%
    mutate(institutionCode = "INBO") %>%
    mutate(datasetName = case_when(.$species_code %in% c("lbbg", "hg") ~ "Bird tracking - GPS tracking of Lesser Black-backed Gulls and Herring Gulls breeding at the southern North Sea coast", .$species_code == "wmh" ~ "Bird tracking - GPS tracking of Western Marsh Harriers breeding near the Belgium-Netherlands border")) %>%
    mutate(basisOfRecord = "MachineObservation") %>%
    mutate(informationWithheld = "see metadata") %>%
    mutate(dynamicProperties = paste("{\"device_info_serial\":", device_info_serial, ", \"catch_location\":\"", catch_location, "\", \"tracking_started_at\":\"", format(tracking_started_at,"%Y-%m-%d"), "\", \"heading_in_degrees\":", str_replace_na(sprintf("%.3f", round(direction, digits = 3)), "null"), "}", sep = "")) %>%

    ## Occurrence terms
    select(-sex, everything()) %>% # This field already exist, so just move it to correct place
    mutate(lifeStage = "adult") %>%

    ## Organism terms
    mutate(organismID = ring_code) %>%
    mutate(organismName = bird_name) %>%

    ## Event terms
    mutate(samplingProtocol = "https://doi.org/10.1007/s10336-012-0908-1") %>%
    mutate(samplingEffort = paste("{\"seconds_since_last_occurrence\":", str_replace_na(calc_time_diff, "null"), "}", sep = "")) %>%
    mutate(eventDate = format(date_time,"%Y-%m-%dT%H:%M:%SZ")) %>%

    ## Location terms
    mutate(minimumElevationInMeters = as.integer(0)) %>%
    mutate(minimumDistanceAboveSurfaceInMeters = as.integer(round(altitude, digits = 0))) %>%
    mutate(decimalLatitude = sprintf("%.7f", round(latitude, digits = 7))) %>%
    mutate(decimalLongitude = sprintf("%.7f", round(longitude, digits = 7))) %>%
    mutate(geodeticDatum = "WGS84") %>%
    mutate(coordinateUncertaintyInMeters = as.integer(str_replace_na(round(h_accuracy, digits = 0), 30))) %>% # Assume 30 meters if no h_accuracy is recorded
    mutate(georeferenceSources = "GPS") %>%
    mutate(georeferenceVerificationStatus = "unverified") %>%

    ## Taxon terms
    mutate(scientificName = scientific_name) %>%
    mutate(kingdom = "Animalia") %>%
    mutate(taxonRank = "species") %>%
    mutate(vernacularName = case_when(.$species_code == "lbbg" ~ "Lesser Black-backed Gull", .$species_code == "hg" ~ "Herring Gull", .$species_code == "wmh" ~ "Western Marsh Harrier")) %>%

    # Assume NA values for calc_outlier are not outliers
    # https://github.com/inbo/bird-tracking-etl/issues/12
    mutate(calc_outlier = coalesce(calc_outlier, FALSE)) %>%

    # Remove source columns (except device_info_serial, species_code, and calc_outlier)
    select(-calc_corine_value, -project_leader, -bird_name, -ring_code, -colour_ring_code, -scientific_name, -catch_weight, -catch_location, -tracking_started_at, -tracking_ended_at, -is_active, -remarks, -colony_latitude, -colony_longitude, -date_time, -latitude, -longitude, -altitude, -pressure, -temperature, -satellites_used, -gps_fixtime, -positiondop, -h_accuracy, -v_accuracy, -x_speed, -y_speed, -z_speed, -speed_accuracy, -userflag, -speed_3d, -speed_2d, -direction, -altitude_agl, -calc_year, -calc_month, -calc_hour, -calc_time_diff, -calc_distance_diff, -calc_speed_2d, -calc_distance_to_colony, -calc_sunlight, -calc_corine_legend)
    ```

3. Once loaded, the number of records should be the same as the number of lines in the source file (minus the header), e.g. in shell:

    ```bash
    wc -l 2017-04-03-processed-logs.csv
    ```

4. Verify columns for `NA` values: there should be none, otherwise replace with empty strings (see script below).
5. Filter the records for publication (either in the UI of Exploratory or by extending the script)

    * For bird-tracking-gull-occurrences

        ```r
        # Replace NA with blank
        mutate(sex = str_replace_na(sex, "")) %>%
        mutate(minimumDistanceAboveSurfaceInMeters = str_replace_na(minimumDistanceAboveSurfaceInMeters, "")) %>%
        
        # Remove outliers
        filter(!calc_outlier) %>%

        # Filter on gulls
        filter(species_code %in% c("hg", "lbbg")) %>%
        # Set an enddate
        filter(eventDate <= as.POSIXct("2016-08-31 23:59:59")) %>%
        
        # Remove non-Darwin Core fields
        select(-device_info_serial, -species_code, -calc_outlier)
        ```
  
    * For bird-tracking-wmh-occurrences

        ```r
        # Replace NA with blank
        mutate(sex = str_replace_na(sex, "")) %>%
        mutate(minimumDistanceAboveSurfaceInMeters = str_replace_na(minimumDistanceAboveSurfaceInMeters, "")) %>%
        
        # Remove outliers
        filter(!calc_outlier) %>%
        
        # Filter on harriers
        filter(species_code == "wmh") %>%
        
        # Remove non-Darwin Core fields
        select(-device_info_serial, -species_code, -calc_outlier)
        ```

6. Export the data as a comma-separated csv with the name `bird_tracking.csv`.
7. Zip the file.
8. Upload the file to the IPT as a new source and verify the Darwin Core mapping.

## Handling file upload limits

The [IPT only accepts uploads (including zips) of less than 100MB](https://github.com/gbif/ipt/wiki/IPT2ManualNotes.wiki#file-upload). If the resulting `bird_tracking.csv.zip` is larger than that, there are two work-arounds for this:

### Upload file directly on server (recommended)

[This issue](https://github.com/gbif/ipt/issues/1300) describes how files can be uploaded directly on the server.

1. Create a [smaller file with 100 records](dwc-occurrence-100.csv):

    ```bash
    head -n 101 bird_tracking.csv > dwc-occurrence-100.csv
    ```
2. Copy and rename the file to `bird_tracking.txt` (note the extension).
3. Login to the IPT, remove any previous source files and upload the file.
4. Map to Darwin Core (should be automapping for all fields). This can also be done as the last step.
5. From local, secure copy the original data file to the IPT server, in a folder you have access to (i.e. just above `$IPT_DATA_DIR`):
    
    ```bash
    scp bird_tracking.csv.zip user@server:path
    ```

6. Login to the IPT server and unzip the file (don't forget to remove the zip and any artefacts):

    ```bash
    unzip bird_tracking.csv.zip
    ```

7. Move the file to the correct resource, replacing the sample file:

    ```
    sudo mv bird_tracking.csv $IPT_DATA_DIR/resources/[shortname]/sources/bird_tracking.txt
    ```

8. In the IPT, click `Edit` next to source, then `Analyze` (might take a while) and `Save` to get the correct metrics.

### Split the file in chunks (not recommended)

1. Copy and execute this bash script ([inspired by this](http://stackoverflow.com/questions/1411713/how-to-split-a-file-and-keep-the-first-line-in-each-of-the-pieces)) in the terminal to split the original file in chunks of a 3 million lines (this might take a while):

    ```bash
    input_file=bird_tracking.csv
    tail -n +2 $input_file | split -l 3000000 - bird_tracking_
    for file in bird_tracking_*
    do
        head -n 1 $input_file > tmp_file
        cat $file >> tmp_file
        mv -f tmp_file $file
        mv $file "$file.csv"
    done
    ```
2. Individually zip the resulting `bird_tracking_aa.csv`, `bird_tracking_ab.csv`, etc. files.
3. Verify none of these are more than 100MB. If they are, use less lines per chunk.
4. Upload the files one by one to the IPT.
5. Map the files to Darwin Core (should be automapping for all fields). *The order in which they are mapped is also the order in which they will be concatenated ([see this issue](https://github.com/gbif/ipt/issues/1339)).*

## Republish

1. Update the metadata where appropriate (e.g. temporal coverage).
2. Republish the dataset.
3. Tell the world! 🎉
